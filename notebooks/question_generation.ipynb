{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer\n",
    "from transformers import logging, AutoConfig, AutoTokenizer, AutoModel\n",
    "logging.set_verbosity_error()\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# alternate: bert-large-uncased (better summary)\n",
    "custom_config = AutoConfig.from_pretrained('mrm8488/bert-tiny-finetuned-squadv2')\n",
    "custom_config.output_hidden_states=True\n",
    "custom_tokenizer = AutoTokenizer.from_pretrained('mrm8488/bert-tiny-finetuned-squadv2')\n",
    "custom_model = AutoModel.from_pretrained('mrm8488/bert-tiny-finetuned-squadv2', config=custom_config)\n",
    "\n",
    "model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.[377] Thousands of seals from the Indus Valley Civilization of the third millennium BCE have been found, usually carved with animals, but a few with human figures. The \"Pashupati\" seal, excavated in Mohenjo-daro, Pakistan, in 1928–29, is the best known.[378][379] After this there is a long period with virtually nothing surviving.[379][380] Almost all surviving ancient Indian art thereafter is in various forms of religious sculpture in durable materials, or coins.\n"
     ]
    }
   ],
   "source": [
    "f = open(\"sample.txt\",\"r\")\n",
    "full_text = f.read()\n",
    "result = model(full_text, min_length=60, max_length = 500 , ratio = 0.4)\n",
    "summarized_text = ''.join(result)\n",
    "print (summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"here are the letters the Romans gave us the near the countries whose languages derived from Latin today they cover half the world as for the ancient Romans the boundaries of the states encompassed their entire civilization the Roman peace or Pax Romana serves as the first example of globalization let's take a walk across 12 centuries of the Roman history and yes those numerals are also a Roman legacy what is Rome City on seven hills capital of Italy but that is today two thousand years ago there it is another thousand years ago they're a tiny tribal settlement of the Latins by the river Tiber how did this manage to conquer the world first it was lucky with its neighbors to the north the Etruscans of modern Tuscany and mysterious people whose language has never been fully decipher to the south Greek colonies these peoples all traded with each other it was at the crossroads of their trade routes that Rome appeared from the very start bromas villain open city a safe haven for outcasts murderers runaway slaves Rome offered migrants a unique opportunity to become fully fledged citizens this will make Rome the largest metropolitan city of the ancient world the Romans themselves believed there were descendants of refugees from the Middle East who had survived the Trojan War Romulus and Remus were the great grandchildren of the Trojan hero Aeneas nursed by an Italian she-wolf the brothers quarrelled we're to cite the future world capital Romulus killed Remus gave his name to the city and became its first ruler as legend has it there were seven kings each of which enjoyed a lengthy reign and left some beneficial legacy a calendar a Sioux system or the capitolium a temple to the senior god Jupiter much of what the Romans later became famous for aqueducts bridges perhaps even the gladiatorial games were borrowed from the this people had invented the Latin alphabet by adapting the Greek letters for their own needs as the surprising that their last Kings were Etruscan Rome borrowed her military and government organization from them while maintaining her Stern patriarchal simplicity in 509 Rome was shaken by a sex scandal the son of King Tarquin the proud raped the chaste Lucretia taco was expelled making him the last king of Rome the Romans decided to prevent any such concentration of power ever again from 509 onwards they elect two consuls to serve a European instead of a monarch the consuls were controlled by the Senate this consisted of three hundred fathers Patras in latin hence the term patricians those not so lucky to be born into the right family x' joining the plebs even if they were as rich as patricians they were not entitled to take up positions in the state it is in the 200 years struggle for these rights that the republic literally meaning the public claim will be formed the praveen's would make up the backbone of the army that they have their own way they would threaten the fledgling state that immigration to a neighboring hill each time the scared patricians caved in introducing for instance the special position of a representative or Tribune of the plebeians these had the right to veto any decisions of the concepts one of the main achievements of this flood was the publication of the first written laws by 287 BC the plebeians had achieved complete equality of rights the unity of Rome found its best expression in the formula sonatas Papa loose clay Romanus the Senate and the Roman people which still adorns the manhole covers in Rome in 3 900 BC the history of Rome could have come to an end the city was unexpectedly taken by the dolls the guard dogs had sense no danger for which they would be crucified every year six geese a walk the last protectors of the Capitoline hill fortress instead saving Rome from complete destruction the shaken Romans conduct that \""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract N (best) nouns from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[127], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nltk\u001b[39m.\u001b[39;49mdownload()\n\u001b[1;32m      3\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpke\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/downloader.py:763\u001b[0m, in \u001b[0;36mDownloader.download\u001b[0;34m(self, info_or_id, download_dir, quiet, force, prefix, halt_on_error, raise_on_error, print_error_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[39mif\u001b[39;00m download_dir \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download_dir \u001b[39m=\u001b[39m download_dir\n\u001b[0;32m--> 763\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interactive_download()\n\u001b[1;32m    764\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    767\u001b[0m     \u001b[39m# Define a helper function for displaying output:\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/downloader.py:1117\u001b[0m, in \u001b[0;36mDownloader._interactive_download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         DownloaderShell(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39mrun()\n\u001b[1;32m   1116\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1117\u001b[0m     DownloaderShell(\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mrun()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/downloader.py:1141\u001b[0m, in \u001b[0;36mDownloaderShell.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simple_interactive_menu(\n\u001b[1;32m   1134\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39md) Download\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1135\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39ml) List\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mq) Quit\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1140\u001b[0m     )\n\u001b[0;32m-> 1141\u001b[0m     user_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39mDownloader> \u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m   1142\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m user_input:\n\u001b[1;32m   1143\u001b[0m         \u001b[39mprint\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import pke\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_nouns(text, n):\n",
    "  text = text.lower()\n",
    "  \n",
    "  extractor = pke.unsupervised.MultipartiteRank()\n",
    "  stoplist = list(string.punctuation)\n",
    "  stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "  stoplist += stopwords.words('english')\n",
    "  extractor.load_document(input=text, stoplist=stoplist)\n",
    "\n",
    "  pos = {'PROPN'}\n",
    "\n",
    "  extractor.candidate_selection(pos=pos)\n",
    "  max_n = len(extractor.candidates)\n",
    "\n",
    "  if (len(extractor.candidates) < max_n):\n",
    "    return [i for i in extractor.candidates if i in text]\n",
    "\n",
    "  for i in range(1, max_n+1):\n",
    "    extractor.candidate_weighting(alpha=1.1,\n",
    "                                    threshold=0.75,\n",
    "                                    method='average')\n",
    "    nouns = [i[0] for i in extractor.get_n_best(i) if i[0] in text]\n",
    "    if len(nouns) == n:\n",
    "      return nouns\n",
    "\n",
    "  return nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_keys = get_n_nouns(full_text, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chiragmanjeshwar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hindu': ['India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.'], 'ellora': [], 'karla': [], 'east': ['India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.', 'India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.']}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from flashtext import KeywordProcessor\n",
    "def tokenize_sentences(text):\n",
    "    sentences = [sent_tokenize(text)]\n",
    "    sentences = [y for x in sentences for y in x]\n",
    "    # Remove any short sentences less than 20 letters.\n",
    "    sentences = [sentence.strip() for sentence in sentences if 20 < len(sentence) < 300]\n",
    "    return sentences\n",
    "def get_sentences_for_keyword(keywords, sentences):\n",
    "    keyword_processor = KeywordProcessor()\n",
    "    keyword_sentences = {}\n",
    "    for word in keywords:\n",
    "        keyword_sentences[word] = []\n",
    "        keyword_processor.add_keyword(word)\n",
    "    for sentence in sentences:\n",
    "        keywords_found = keyword_processor.extract_keywords(sentence)\n",
    "        for key in keywords_found:\n",
    "            keyword_sentences[key].append(sentence)\n",
    "    for key in keyword_sentences.keys():\n",
    "        values = keyword_sentences[key]\n",
    "        values = sorted(values, key=len, reverse=True)\n",
    "        keyword_sentences[key] = values\n",
    "    return keyword_sentences\n",
    "sentences = tokenize_sentences(summarized_text)\n",
    "keyword_sentence_mapping = get_sentences_for_keyword(get_n_nouns(full_text, 4), sentences)    \n",
    "print (keyword_sentence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_n(text):\n",
    "  extractor = pke.unsupervised.MultipartiteRank()\n",
    "  stoplist = list(string.punctuation)\n",
    "  stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "  stoplist += stopwords.words('english')\n",
    "  extractor.load_document(input=text, stoplist=stoplist)\n",
    "\n",
    "  pos = {'PROPN'}\n",
    "\n",
    "  extractor.candidate_selection(pos=pos)\n",
    "  max_n = len(extractor.candidates)\n",
    "\n",
    "  return max_n\n",
    "\n",
    "import functools\n",
    "def get_size(keyword_sentence_mapping):\n",
    "  return functools.reduce(lambda x, y: x + len(y), keyword_sentence_mapping.values(), 0)\n",
    "\n",
    "def get_n_sentence_mapping(text, n):\n",
    "\n",
    "  max_n = get_max_n(text)\n",
    "  for i in range(1, max_n):\n",
    "    sentences = tokenize_sentences(summarized_text)\n",
    "    keyword_sentence_mapping = get_sentences_for_keyword(get_n_nouns(full_text, i), sentences)\n",
    "    if get_size(keyword_sentence_mapping) == n:\n",
    "      break\n",
    "  \n",
    "  return keyword_sentence_mapping\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_size(keyword_sentence_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hindu': ['India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.'],\n",
       " 'ellora': [],\n",
       " 'karla': [],\n",
       " 'east': ['India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.',\n",
       "  'India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.']}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_sentence_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up PyWSD (takes ~10 secs)... "
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/chiragmanjeshwar/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[126], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpywsd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msimilarity\u001b[39;00m \u001b[39mimport\u001b[39;00m max_similarity\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpywsd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlesk\u001b[39;00m \u001b[39mimport\u001b[39;00m adapted_lesk\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpywsd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlesk\u001b[39;00m \u001b[39mimport\u001b[39;00m simple_lesk\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pywsd/__init__.py:34\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39m#import semcor\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m#import semeval\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpywsd\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mallwords_wsd\u001b[39;00m \u001b[39mimport\u001b[39;00m disambiguate\n\u001b[0;32m---> 34\u001b[0m simple_lesk(\u001b[39m'\u001b[39;49m\u001b[39mThis is a foo bar sentence\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mbar\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     35\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtook \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m secs.\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime()\u001b[39m-\u001b[39mstart), file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pywsd/lesk.py:241\u001b[0m, in \u001b[0;36msimple_lesk\u001b[0;34m(context_sentence, ambiguous_word, pos, lemma, stem, hyperhypo, stop, context_is_lemmatized, nbest, keepscore, normalizescore, from_cache)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mSimple Lesk is somewhere in between using more than the\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[39moriginal Lesk algorithm (1986) and using less signature\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39m:return: A Synset for the estimated best sense.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m# Ensure that ambiguous word is a lemma.\u001b[39;00m\n\u001b[0;32m--> 241\u001b[0m ambiguous_word \u001b[39m=\u001b[39m lemmatize(ambiguous_word, pos\u001b[39m=\u001b[39;49mpos)\n\u001b[1;32m    242\u001b[0m \u001b[39m# If ambiguous word not in WordNet return None\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m wn\u001b[39m.\u001b[39msynsets(ambiguous_word):\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pywsd/utils.py:92\u001b[0m, in \u001b[0;36mlemmatize\u001b[0;34m(ambiguous_word, pos, neverstem, lemmatizer, stemmer)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39mTries to convert a surface word into lemma, and if lemmatize word is not in\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39mwordnet then try and convert surface word into its stem.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mword and the surface word is a not a lemma.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[39m# Try to be a little smarter and use most frequent POS.\u001b[39;00m\n\u001b[0;32m---> 92\u001b[0m pos \u001b[39m=\u001b[39m pos \u001b[39mif\u001b[39;00m pos \u001b[39melse\u001b[39;00m penn2morphy(pos_tag([ambiguous_word])[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m],\n\u001b[1;32m     93\u001b[0m                                  default_to_noun\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     94\u001b[0m lemma \u001b[39m=\u001b[39m lemmatizer\u001b[39m.\u001b[39mlemmatize(ambiguous_word, pos\u001b[39m=\u001b[39mpos)\n\u001b[1;32m     95\u001b[0m stem \u001b[39m=\u001b[39m stemmer\u001b[39m.\u001b[39mstem(ambiguous_word)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/tag/__init__.py:165\u001b[0m, in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpos_tag\u001b[39m(tokens, tagset\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meng\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    141\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39m    tag the given list of tokens.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     tagger \u001b[39m=\u001b[39m _get_tagger(lang)\n\u001b[1;32m    166\u001b[0m     \u001b[39mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/tag/__init__.py:107\u001b[0m, in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    105\u001b[0m     tagger\u001b[39m.\u001b[39mload(ap_russian_model_loc)\n\u001b[1;32m    106\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     tagger \u001b[39m=\u001b[39m PerceptronTagger()\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m tagger\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/tag/perceptron.py:167\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m()\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m load:\n\u001b[1;32m    166\u001b[0m     AP_MODEL_LOC \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\n\u001b[0;32m--> 167\u001b[0m         find(\u001b[39m\"\u001b[39;49m\u001b[39mtaggers/averaged_perceptron_tagger/\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m+\u001b[39;49m PICKLE)\n\u001b[1;32m    168\u001b[0m     )\n\u001b[1;32m    169\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload(AP_MODEL_LOC)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger/averaged_perceptron_tagger.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/chiragmanjeshwar/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/share/nltk_data'\n    - '/opt/homebrew/opt/python@3.11/Frameworks/Python.framework/Versions/3.11/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import re\n",
    "import random\n",
    "from pywsd.similarity import max_similarity\n",
    "from pywsd.lesk import adapted_lesk\n",
    "from pywsd.lesk import simple_lesk\n",
    "from pywsd.lesk import cosine_lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Distractors from Wordnet\n",
    "def get_distractors_wordnet(syn,word):\n",
    "    distractors=[]\n",
    "    word= word.lower()\n",
    "    orig_word = word\n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    hypernym = syn.hypernyms()\n",
    "    if len(hypernym) == 0: \n",
    "        return distractors\n",
    "    for item in hypernym[0].hyponyms():\n",
    "        name = item.lemmas()[0].name()\n",
    "        #print (\"name \",name, \" word\",orig_word)\n",
    "        if name == orig_word:\n",
    "            continue\n",
    "        name = name.replace(\"_\",\" \")\n",
    "        name = \" \".join(w.capitalize() for w in name.split())\n",
    "        if name is not None and name not in distractors:\n",
    "            distractors.append(name)\n",
    "    return distractors\n",
    "\n",
    "def get_wordsense(sent,word):\n",
    "    word= word.lower()\n",
    "    \n",
    "    if len(word.split())>0:\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    \n",
    "    \n",
    "    synsets = wn.synsets(word,'n')\n",
    "    if synsets:\n",
    "        wup = max_similarity(sent, word, 'wup', pos='n')\n",
    "        adapted_lesk_output =  adapted_lesk(sent, word, pos='n')\n",
    "        lowest_index = min (synsets.index(wup),synsets.index(adapted_lesk_output))\n",
    "        return synsets[lowest_index]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Distractors from http://conceptnet.io/\n",
    "def get_distractors_conceptnet(word):\n",
    "    word = word.lower()\n",
    "    original_word= word\n",
    "    if (len(word.split())>0):\n",
    "        word = word.replace(\" \",\"_\")\n",
    "    distractor_list = [] \n",
    "    url = \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n",
    "    obj = requests.get(url).json()\n",
    "\n",
    "    for edge in obj['edges']:\n",
    "        link = edge['end']['term'] \n",
    "\n",
    "        url2 = \"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n",
    "        obj2 = requests.get(url2).json()\n",
    "        for edge in obj2['edges']:\n",
    "            word2 = edge['start']['label']\n",
    "            if word2 not in distractor_list and original_word.lower() not in word2.lower():\n",
    "                distractor_list.append(word2)\n",
    "                   \n",
    "    return distractor_list\n",
    "\n",
    "key_distractor_list = {}\n",
    "\n",
    "for keyword in keyword_sentence_mapping:\n",
    "    wordsense = get_wordsense(keyword_sentence_mapping[keyword][0],keyword)\n",
    "    if wordsense:\n",
    "        distractors = get_distractors_wordnet(wordsense,keyword)\n",
    "        if len(distractors) ==0:\n",
    "            distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "    else:\n",
    "        \n",
    "        distractors = get_distractors_conceptnet(keyword)\n",
    "        if len(distractors) != 0:\n",
    "            key_distractor_list[keyword] = distractors\n",
    "\n",
    "index = 1\n",
    "print (\"#############################################################################\")\n",
    "print (\"NOTE::::::::  Since the algorithm might have errors along the way, wrong answer choices generated might not be correct for some questions. \")\n",
    "print (\"#############################################################################\\n\\n\")\n",
    "for each in key_distractor_list:\n",
    "    sentence = keyword_sentence_mapping[each][0]\n",
    "    pattern = re.compile(each, re.IGNORECASE)\n",
    "    output = pattern.sub( \" _______ \", sentence)\n",
    "    print (\"%s)\"%(index),output)\n",
    "    choices = [each.capitalize()] + key_distractor_list[each]\n",
    "    top4choices = choices[:4]\n",
    "    random.shuffle(top4choices)\n",
    "    optionchoices = ['a','b','c','d']\n",
    "    for idx,choice in enumerate(top4choices):\n",
    "        print (\"\\t\",optionchoices[idx],\")\",\" \",choice)\n",
    "    print (\"\\nMore options: \", choices[4:20],\"\\n\\n\")\n",
    "    index = index + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'India has a very ancient tradition of art, which has exchanged many influences with the rest of Eurasia, especially in the first millennium, when Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.[377] Thousands of seals from the Indus Valley Civilization of the third millennium BCE have been found, usually carved with animals, but a few with human figures. The \"Pashupati\" seal, excavated in Mohenjo-daro, Pakistan, in 1928–29, is the best known.[378][379] After this there is a long period with virtually nothing surviving.[379][380] Almost all surviving ancient Indian art thereafter is in various forms of religious sculpture in durable materials, or coins. There was probably originally far more in wood, which is lost. In north India Mauryan art is the first imperial movement.[381][382][383] In the first millennium CE, Buddhist art spread with Indian religions to Central, East and South-East Asia, the last also greatly influenced by Hindu art.[384] Over the following centuries a distinctly Indian style of sculpting the human figure developed, with less interest in articulating precise anatomy than ancient Greek sculpture but showing smoothly-flowing forms expressing prana (\"breath\" or life-force).[385][386] This is often complicated by the need to give figures multiple arms or heads, or represent different genders on the left and right of figures, as with the Ardhanarishvara form of Shiva and Parvati.[387][388]\\n\\nMost of the earliest large sculpture is Buddhist, either excavated from Buddhist stupas such as Sanchi, Sarnath and Amaravati,[389] or is rock-cut reliefs at sites such as Ajanta, Karla and Ellora. Hindu and Jain sites appear rather later.[390][391] In spite of this complex mixture of religious traditions, generally, the prevailing artistic style at any time and place has been shared by the major religious groups, and sculptors probably usually served all communities.[392] Gupta art, at its peak c.\\u2009300 CE – c.\\u2009500 CE, is often regarded as a classical period whose influence lingered for many centuries after; it saw a new dominance of Hindu sculpture, as at the Elephanta Caves.[393][394] Across the north, this became rather stiff and formulaic after c.\\u2009800 CE, though rich with finely carved detail in the surrounds of statues.[395] But in the South, under the Pallava and Chola dynasties, sculpture in both stone and bronze had a sustained period of great achievement; the large bronzes with Shiva as Nataraja have become an iconic symbol of India.[396][397]\\n\\nAncient painting has only survived at a few sites, of which the crowded scenes of court life in the Ajanta Caves are by far the most important, but it was evidently highly developed, and is mentioned as a courtly accomplishment in Gupta times.[398][399] Painted manuscripts of religious texts survive from Eastern India about the 10th century onwards, most of the earliest being Buddhist and later Jain. No doubt the style of these was used in larger paintings.[400] The Persian-derived Deccan painting, starting just before the Mughal miniature, between them give the first large body of secular painting, with an emphasis on portraits, and the recording of princely pleasures and wars.[401][402] The style spread to Hindu courts, especially among the Rajputs, and developed a variety of styles, with the smaller courts often the most innovative, with figures such as Nihâl Chand and Nainsukh.[403][404] As a market developed among European residents, it was supplied by Company painting by Indian artists with considerable Western influence.[405][406] In the 19th century, cheap Kalighat paintings of gods and everyday life, done on paper, were urban folk art from Calcutta, which later saw the Bengal School of Art, reflecting the art colleges founded by the British, the first movement in modern Indian painting.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
